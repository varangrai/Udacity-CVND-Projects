{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpYcYtrkCnie"
   },
   "source": [
    "# Project: Image Captioning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEVrDqGzCt22"
   },
   "source": [
    "## Notebook 2: Data Preprocessing and Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNfXIe3oCnig"
   },
   "source": [
    "<a id='step1'></a>\n",
    "## 1: Loading the Data Loader\n",
    "\n",
    "In the code cell below, you will initialize the data loader by using the `get_loader` function in **data_loader.py**.  \n",
    "``data_loader`` will be used to load the COCO dataset in batches. \n",
    "\n",
    "\n",
    "`get_loader` function takes as input a number of arguments that can be explored in **data_loader.py**.  \n",
    "\n",
    "Main arguements that it takes are:\n",
    "1. **`transform`** - an [image transform](http://pytorch.org/docs/master/torchvision/transforms.html) specifying how to pre-process the images and convert them to PyTorch tensors before using them as input to the CNN encoder.\n",
    "2. **`mode`** - one of `'train'` (loads the training data in batches) or `'test'` (for the test data).\n",
    "3. **`batch_size`** - determines the batch size.\n",
    "4. **`vocab_threshold`** - the total number of times that a word must appear in the in the training captions before it is used as part of the vocabulary.  Words that have fewer than `vocab_threshold` occurrences in the training captions are considered unknown words. \n",
    "5. **`vocab_from_file`** - a Boolean that decides whether to load the vocabulary from file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9d4RumaCnih",
    "outputId": "644f7fd2-bc69-4cea-b63a-86ab2fba40d0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\envs\\cv-nd\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\cv-nd\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\programdata\\anaconda3\\envs\\cv-nd\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rodrigo\n",
      "[nltk_data]     Franco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.74s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.81s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 414113/414113 [00:49<00:00, 8415.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          \n",
    "    transforms.RandomCrop(224),                      \n",
    "    transforms.RandomHorizontalFlip(),               \n",
    "    transforms.ToTensor(),                           \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "vocab_threshold = 5         \n",
    "batch_size = 10\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYlwcQ5lCniq"
   },
   "source": [
    "### Exploring the `__getitem__` Method in CoCoDataset\n",
    "\n",
    "The `__getitem__` method in the `CoCoDataset` class determines how an image-caption pair is pre-processed before being incorporated into a batch.\n",
    "\n",
    "When the data loader is in training mode, this method begins by first obtaining the filename (`path`) of a training image and its corresponding caption (`caption`).\n",
    "\n",
    "#### Image Pre-Processing \n",
    "\n",
    "Image pre-processing is defined in the `__getitem__` method in the `CoCoDataset` class):\n",
    "\n",
    "After loading the image in the training folder with name `path`, the image is pre-processed using the same transform (`transform_train`) that was supplied when instantiating the data loader.  \n",
    "\n",
    "#### Caption Pre-Processing \n",
    "\n",
    "The captions also need to be pre-processed and prepped for training. In this example, for generating captions, we are aiming to create a model that predicts the next token of a sentence from previous tokens, so we turn the caption associated with any image into a list of tokenized words, before casting it to a PyTorch tensor that we can use to train the network.\n",
    "\n",
    "String-valued captions are converted to a list of integers, before casting it to a PyTorch tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYuIv3wACnis"
   },
   "outputs": [],
   "source": [
    "sample_caption = 'A person doing a trick on a rail while riding a skateboard.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "czmSiOiACni0",
    "outputId": "046546a1-1023-4a8d-b1fc-fdc32d67f6b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'person', 'doing', 'a', 'trick', 'on', 'a', 'rail', 'while', 'riding', 'a', 'skateboard', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sample_tokens = nltk.tokenize.word_tokenize(str(sample_caption).lower())\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piPdVsUxCni4"
   },
   "source": [
    "The integer `0` is always used to mark the start of a caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUqkrqkOCni4",
    "outputId": "91054a94-99ff-4479-8d32-df13e8386124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special start word: <start>\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "sample_caption = []\n",
    "start_word = data_loader.dataset.vocab.start_word\n",
    "print('Special start word:', start_word)\n",
    "sample_caption.append(data_loader.dataset.vocab(start_word))\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5GapyCICnjC",
    "outputId": "29483940-d337-4b66-f004-dffda566495d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 98, 754, 3, 396, 39, 3, 1009, 207, 139, 3, 753, 18]\n"
     ]
    }
   ],
   "source": [
    "sample_caption.extend([data_loader.dataset.vocab(token) for token in sample_tokens])\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0NF9GFMCnjO"
   },
   "source": [
    "As you will see below, the integer `1` is always used to  mark the end of a caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sOBoZf4DCnjO",
    "outputId": "f1df075c-ae77-40fe-d70c-ade8e703aa5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special end word: <end>\n",
      "[0, 3, 98, 754, 3, 396, 39, 3, 1009, 207, 139, 3, 753, 18, 1]\n"
     ]
    }
   ],
   "source": [
    "end_word = data_loader.dataset.vocab.end_word\n",
    "print('Special end word:', end_word)\n",
    "\n",
    "sample_caption.append(data_loader.dataset.vocab(end_word))\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esg_CHQvCnjZ"
   },
   "source": [
    "Converting to Pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVFCQAj9Cnja",
    "outputId": "4aa7fa2d-ffbd-46f1-e401-4d733a1cedc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    3,   98,  754,    3,  396,   39,    3, 1009,  207,  139,    3,\n",
      "         753,   18,    1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sample_caption = torch.Tensor(sample_caption).long()\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSXZiTeXCnjg",
    "outputId": "7827ed84-86d2-454c-e2d2-68b2b6c9356e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<start>': 0,\n",
       " '<end>': 1,\n",
       " '<unk>': 2,\n",
       " 'a': 3,\n",
       " 'very': 4,\n",
       " 'clean': 5,\n",
       " 'and': 6,\n",
       " 'well': 7,\n",
       " 'decorated': 8,\n",
       " 'empty': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the word2idx dictionary.\n",
    "dict(list(data_loader.dataset.vocab.word2idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7ezZetvCnjn",
    "outputId": "e20a4827-9525-4eba-b42f-c73a84ebfb15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in vocabulary: 8856\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of keys in the word2idx dictionary.\n",
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "myUnxqDSCnjs"
   },
   "source": [
    "In **vocabulary.py**, the `word2idx` dictionary is created by looping over the captions in the training dataset.  If a token appears no less than `vocab_threshold` times in the training set, then it is added as a key to the dictionary and assigned a corresponding unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvn-MIpYCnjt",
    "outputId": "8b7e554d-5e80-4df8-d111-f4490bb1a8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.84s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.77s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 414113/414113 [00:49<00:00, 8296.06it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_threshold = 4\n",
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzO6XdWlCnj2",
    "outputId": "c2d23e46-28c6-4044-f4cd-d2d8a8ced563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in vocabulary: 9955\n"
     ]
    }
   ],
   "source": [
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n0Qye8R3Cnj5"
   },
   "source": [
    "There are also a few special keys in the `word2idx` dictionary.:\n",
    "- special start word is (`\"<start>\"`)\n",
    "- special end word (`\"<end>\"`). \n",
    "- corresponding to unknown words (`\"<unk>\"`). \n",
    "- any unknown tokens are mapped to the integer `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUp6kTq-Cnj6",
    "outputId": "26149591-edce-4351-e964-2975dabd3966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special unknown word: <unk>\n",
      "All unknown words are mapped to this integer: 2\n"
     ]
    }
   ],
   "source": [
    "unk_word = data_loader.dataset.vocab.unk_word\n",
    "print('Special unknown word:', unk_word)\n",
    "\n",
    "print('All unknown words are mapped to this integer:', data_loader.dataset.vocab(unk_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uKctlwIwCnkA"
   },
   "source": [
    "Checking ``'unk`` token index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YaoJH51TCnkB",
    "outputId": "bf14fb43-2e98-42ff-db56-23c6eec1af18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(data_loader.dataset.vocab('rodrigo'))\n",
    "print(data_loader.dataset.vocab('ieowoqjf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhxerI17CnkE"
   },
   "source": [
    "The vocabulary (`data_loader.dataset.vocab`) is saved as a pickle file in the project folder, with filename `vocab.pkl`.\n",
    "\n",
    "`vocab_from_file=True` : This will load from pkl file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0EN0H3NCnkF",
    "outputId": "922f6f2b-6e90-49cd-b661-7f7d5840db3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.77s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 414113/414113 [00:50<00:00, 8166.28it/s]\n"
     ]
    }
   ],
   "source": [
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_from_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfOrZfNuCnkK"
   },
   "source": [
    "## 2 :Using the Data Loader to Obtain Batches\n",
    "\n",
    "The captions in the dataset vary greatly in length. \n",
    "\n",
    "In the code cell below,it can be seen that majority of captions have length 10.  Likewise, very short and very long captions are quite rare.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7jQx9aHCnkK",
    "outputId": "8fe12afe-b274-43b4-adec-e707cf81c263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 10 --- count: 86332\n",
      "value: 11 --- count: 79945\n",
      "value:  9 --- count: 71935\n",
      "value: 12 --- count: 57639\n",
      "value: 13 --- count: 37648\n",
      "value: 14 --- count: 22335\n",
      "value:  8 --- count: 20769\n",
      "value: 15 --- count: 12842\n",
      "value: 16 --- count:  7729\n",
      "value: 17 --- count:  4842\n",
      "value: 18 --- count:  3103\n",
      "value: 19 --- count:  2015\n",
      "value:  7 --- count:  1597\n",
      "value: 20 --- count:  1451\n",
      "value: 21 --- count:   999\n",
      "value: 22 --- count:   683\n",
      "value: 23 --- count:   534\n",
      "value: 24 --- count:   383\n",
      "value: 25 --- count:   277\n",
      "value: 26 --- count:   215\n",
      "value: 27 --- count:   159\n",
      "value: 28 --- count:   115\n",
      "value: 29 --- count:    86\n",
      "value: 30 --- count:    58\n",
      "value: 31 --- count:    49\n",
      "value: 32 --- count:    44\n",
      "value: 34 --- count:    39\n",
      "value: 37 --- count:    32\n",
      "value: 33 --- count:    31\n",
      "value: 35 --- count:    31\n",
      "value: 36 --- count:    26\n",
      "value: 38 --- count:    18\n",
      "value: 39 --- count:    18\n",
      "value: 43 --- count:    16\n",
      "value: 44 --- count:    16\n",
      "value: 48 --- count:    12\n",
      "value: 45 --- count:    11\n",
      "value: 42 --- count:    10\n",
      "value: 40 --- count:     9\n",
      "value: 49 --- count:     9\n",
      "value: 46 --- count:     9\n",
      "value: 47 --- count:     7\n",
      "value: 50 --- count:     6\n",
      "value: 51 --- count:     6\n",
      "value: 41 --- count:     6\n",
      "value: 52 --- count:     5\n",
      "value: 54 --- count:     3\n",
      "value: 56 --- count:     2\n",
      "value:  6 --- count:     2\n",
      "value: 53 --- count:     2\n",
      "value: 55 --- count:     2\n",
      "value: 57 --- count:     1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Tally the total number of training captions with each length.\n",
    "counter = Counter(data_loader.dataset.caption_lengths)\n",
    "lengths = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
    "for value, count in lengths:\n",
    "    print('value: %2d --- count: %5d' % (value, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKYtqjBKCnkX"
   },
   "source": [
    "* To generate batches of training data, first sampling a caption length is done. (where the probability that any length is drawn is proportional to the number of captions with that length in the dataset). \n",
    "* Then, `batch_size` number of image-caption pairs are retrieved, where all captions have the sampled length.  This approach for assembling batches matches the procedure in [this paper](https://arxiv.org/pdf/1502.03044.pdf) and has been shown to be computationally efficient without degrading performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEIW8j0hCnkY",
    "outputId": "38f2cb17-dcac-4915-9283-347871a46084",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled indices: [59524, 178926, 316467, 261850, 287304, 381893, 371050, 33572, 272310, 163231]\n",
      "images.shape: torch.Size([10, 3, 224, 224])\n",
      "captions.shape: torch.Size([10, 14])\n",
      "images: tensor([[[[ 0.1939,  0.1939,  0.1939,  ...,  0.2453,  0.2796,  0.2796],\n",
      "          [ 0.1597,  0.0912,  0.1426,  ...,  0.3652,  0.4166,  0.4166],\n",
      "          [ 0.1426,  0.0056,  0.1254,  ...,  0.4337,  0.4166,  0.4508],\n",
      "          ...,\n",
      "          [-0.2171, -0.4739, -0.7479,  ...,  1.2557,  1.1358,  1.0502],\n",
      "          [-0.7308, -0.9020, -1.2274,  ...,  1.2214,  1.1187,  1.0331],\n",
      "          [-1.2274, -1.4672, -1.7412,  ...,  1.2557,  1.1872,  1.0673]],\n",
      "\n",
      "         [[-0.6352, -0.6702, -0.6877,  ..., -0.7052, -0.6352, -0.6527],\n",
      "          [-0.7227, -0.8277, -0.8102,  ..., -0.6001, -0.5301, -0.5301],\n",
      "          [-0.7927, -0.9678, -0.8803,  ..., -0.5651, -0.5651, -0.5651],\n",
      "          ...,\n",
      "          [-0.4951, -0.7227, -0.9678,  ...,  0.3978,  0.3627,  0.2927],\n",
      "          [-0.9153, -1.1078, -1.4055,  ...,  0.3627,  0.3277,  0.2577],\n",
      "          [-1.3704, -1.5980, -1.8782,  ...,  0.3803,  0.3978,  0.2752]],\n",
      "\n",
      "         [[-0.9678, -0.9853, -1.0550,  ..., -1.1247, -1.1247, -1.1596],\n",
      "          [-1.0201, -1.1247, -1.1944,  ..., -1.0898, -1.0724, -1.0898],\n",
      "          [-1.1073, -1.2816, -1.2293,  ..., -1.0898, -1.1421, -1.1421],\n",
      "          ...,\n",
      "          [-0.8633, -1.0376, -1.2641,  ..., -0.6367, -0.6715, -0.7064],\n",
      "          [-1.1596, -1.2467, -1.4733,  ..., -0.6715, -0.7064, -0.7413],\n",
      "          [-1.4384, -1.5430, -1.7347,  ..., -0.6715, -0.6367, -0.7238]]],\n",
      "\n",
      "\n",
      "        [[[-1.5870, -1.6213, -1.4329,  ..., -0.4397, -0.5424, -0.5596],\n",
      "          [-1.5357, -1.5699, -1.4158,  ..., -0.3541, -0.4911, -0.5596],\n",
      "          [-1.5357, -1.6727, -1.4329,  ..., -0.3027, -0.4397, -0.4739],\n",
      "          ...,\n",
      "          [-1.6042, -1.6384, -1.7412,  ..., -1.9295, -1.9124, -1.9638],\n",
      "          [-1.6555, -1.6727, -1.7583,  ..., -1.8439, -1.9124, -1.9467],\n",
      "          [-1.7240, -1.7069, -1.7240,  ..., -1.8782, -1.8953, -1.8610]],\n",
      "\n",
      "         [[-1.5980, -1.6155, -1.6506,  ..., -1.3004, -1.2829, -1.2829],\n",
      "          [-1.6331, -1.6331, -1.6331,  ..., -1.2479, -1.2654, -1.2829],\n",
      "          [-1.6856, -1.6155, -1.6155,  ..., -1.2654, -1.2479, -1.3004],\n",
      "          ...,\n",
      "          [-1.6155, -1.5805, -1.5105,  ..., -1.7906, -1.7906, -1.7731],\n",
      "          [-1.5630, -1.5455, -1.5630,  ..., -1.7381, -1.7556, -1.7556],\n",
      "          [-1.6155, -1.5805, -1.5455,  ..., -1.7556, -1.7731, -1.8081]],\n",
      "\n",
      "         [[-1.5081, -1.4907, -1.4559,  ..., -1.4210, -1.4384, -1.4384],\n",
      "          [-1.4733, -1.4733, -1.4907,  ..., -1.3861, -1.3687, -1.4559],\n",
      "          [-1.4210, -1.4210, -1.5081,  ..., -1.4907, -1.4384, -1.4036],\n",
      "          ...,\n",
      "          [-1.3164, -1.3164, -1.2467,  ..., -1.4559, -1.4733, -1.4733],\n",
      "          [-1.2467, -1.3164, -1.2990,  ..., -1.4733, -1.4733, -1.5081],\n",
      "          [-1.1944, -1.3513, -1.2990,  ..., -1.5256, -1.4733, -1.4907]]],\n",
      "\n",
      "\n",
      "        [[[-1.9809, -1.8610, -1.7583,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [-1.9638, -1.9124, -1.7583,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [-1.9809, -1.9980, -1.8782,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          ...,\n",
      "          [ 0.4166,  1.4269,  1.3927,  ..., -0.8849, -0.8164, -0.7993],\n",
      "          [ 0.8618,  1.4098,  1.4954,  ..., -0.9534, -0.8335, -0.7993],\n",
      "          [ 1.1187,  1.2728,  1.2728,  ..., -0.9534, -0.8164, -0.7479]],\n",
      "\n",
      "         [[-1.8606, -1.7381, -1.5980,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [-1.8606, -1.7906, -1.5805,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [-1.8782, -1.8782, -1.7206,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          ...,\n",
      "          [ 0.2927,  1.4132,  1.4832,  ..., -1.3704, -1.3704, -1.3529],\n",
      "          [ 0.8704,  1.4657,  1.5707,  ..., -1.4230, -1.3880, -1.3529],\n",
      "          [ 1.0630,  1.3606,  1.5007,  ..., -1.4055, -1.3529, -1.3354]],\n",
      "\n",
      "         [[-1.7347, -1.7347, -1.6999,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [-1.6999, -1.7870, -1.7173,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [-1.7347, -1.8044, -1.7696,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          ...,\n",
      "          [ 0.6705,  1.7337,  1.7511,  ..., -1.4907, -1.4733, -1.4559],\n",
      "          [ 1.2282,  1.7511,  1.8208,  ..., -1.5779, -1.4907, -1.4559],\n",
      "          [ 1.4200,  1.6117,  1.6291,  ..., -1.5604, -1.4907, -1.4384]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.0948,  2.0263,  1.9749,  ..., -0.3369,  0.2111,  0.1939],\n",
      "          [ 2.0605,  2.0605,  1.9578,  ..., -0.1999, -0.2684, -0.6281],\n",
      "          [ 2.0948,  2.0777,  1.9749,  ..., -0.4054, -0.5253, -0.5424],\n",
      "          ...,\n",
      "          [-1.5528, -1.5528, -1.4843,  ..., -0.4054, -0.5596, -0.7308],\n",
      "          [-1.6042, -1.5357, -1.5014,  ..., -0.2171, -0.0458, -0.4054],\n",
      "          [-1.6213, -1.5357, -1.5870,  ..., -0.3198, -0.4739, -0.6794]],\n",
      "\n",
      "         [[ 2.3936,  2.3761,  2.3410,  ..., -0.6001, -0.0924, -0.0749],\n",
      "          [ 2.4111,  2.3936,  2.3235,  ..., -0.4076, -0.5301, -0.8277],\n",
      "          [ 2.4111,  2.3761,  2.3060,  ..., -0.6877, -0.8102, -0.7227],\n",
      "          ...,\n",
      "          [-1.8431, -1.8431, -1.8081,  ..., -1.7206, -1.7731, -1.7556],\n",
      "          [-1.8431, -1.8081, -1.7731,  ..., -1.7031, -1.5630, -1.6155],\n",
      "          [-1.8431, -1.7556, -1.8256,  ..., -1.7556, -1.7906, -1.7731]],\n",
      "\n",
      "         [[ 2.6051,  2.5877,  2.5703,  ..., -1.2119, -0.8633, -0.8807],\n",
      "          [ 2.5877,  2.6051,  2.5354,  ..., -0.9156, -1.0550, -1.2990],\n",
      "          [ 2.6051,  2.6051,  2.5006,  ..., -1.1770, -1.1421, -1.1421],\n",
      "          ...,\n",
      "          [-1.6824, -1.6476, -1.5779,  ..., -1.3861, -1.4559, -1.4210],\n",
      "          [-1.6824, -1.6476, -1.5779,  ..., -1.2990, -1.1770, -1.2641],\n",
      "          [-1.6650, -1.6127, -1.6650,  ..., -1.3861, -1.4036, -1.3861]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6667,  1.6495,  1.6667,  ...,  1.5810,  1.5468,  1.5468],\n",
      "          [ 1.6495,  1.6495,  1.6495,  ...,  1.5810,  1.5639,  1.5468],\n",
      "          [ 1.6324,  1.6495,  1.6495,  ...,  1.5810,  1.5810,  1.5639],\n",
      "          ...,\n",
      "          [ 0.2624,  1.1700,  0.6049,  ...,  0.4508,  0.4337,  0.4166],\n",
      "          [ 0.9646,  1.2214,  0.2111,  ...,  0.4166,  0.4166,  0.5193],\n",
      "          [ 0.4679,  1.0673,  0.0569,  ...,  0.4337,  0.4508,  0.6563]],\n",
      "\n",
      "         [[ 1.9559,  1.9384,  1.9559,  ...,  1.9034,  1.8683,  1.8683],\n",
      "          [ 1.9384,  1.9384,  1.9384,  ...,  1.9034,  1.8859,  1.8683],\n",
      "          [ 1.9209,  1.9384,  1.9384,  ...,  1.9034,  1.9034,  1.8859],\n",
      "          ...,\n",
      "          [ 0.0476,  0.9580,  0.6954,  ...,  0.5728,  0.5553,  0.5378],\n",
      "          [ 0.8179,  1.2381,  0.3452,  ...,  0.5203,  0.5203,  0.6078],\n",
      "          [ 0.1527,  1.0630,  0.2227,  ...,  0.5728,  0.5728,  0.7304]],\n",
      "\n",
      "         [[ 2.3088,  2.2914,  2.3088,  ...,  2.2740,  2.2391,  2.2391],\n",
      "          [ 2.2914,  2.2914,  2.2914,  ...,  2.2740,  2.2566,  2.2391],\n",
      "          [ 2.2740,  2.2914,  2.2914,  ...,  2.2740,  2.2740,  2.2566],\n",
      "          ...,\n",
      "          [ 0.0605,  1.0017,  0.6879,  ...,  0.7925,  0.7751,  0.7576],\n",
      "          [ 0.8274,  1.2282,  0.3568,  ...,  0.7402,  0.7228,  0.8099],\n",
      "          [ 0.2173,  1.0539,  0.2173,  ...,  0.7576,  0.7576,  0.9494]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4337,  0.3823,  0.7419,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [ 0.4508,  0.4508,  0.5022,  ..., -2.1179, -2.1179, -2.1008],\n",
      "          [ 0.5193,  0.5193,  0.4679,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          ...,\n",
      "          [-0.5082,  0.2796,  0.9646,  ..., -0.8164, -0.7822, -0.8164],\n",
      "          [-0.4739,  0.3652,  1.0159,  ..., -0.8164, -0.7822, -0.7650],\n",
      "          [-0.4911,  0.4166,  1.0159,  ..., -0.8335, -0.8507, -0.7993]],\n",
      "\n",
      "         [[-1.1253, -1.1429, -0.9328,  ..., -2.0357, -2.0357, -2.0182],\n",
      "          [-1.1954, -1.1954, -1.1253,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-1.1954, -1.2304, -1.1954,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          ...,\n",
      "          [-0.6527,  0.1702,  0.8529,  ..., -1.6331, -1.5280, -1.5455],\n",
      "          [-0.6877,  0.2402,  0.8880,  ..., -1.6155, -1.5980, -1.4755],\n",
      "          [-0.6527,  0.3102,  0.9055,  ..., -1.6331, -1.6681, -1.5805]],\n",
      "\n",
      "         [[-0.5844, -0.5844, -0.4798,  ..., -1.8044, -1.8044, -1.7870],\n",
      "          [-0.6890, -0.7064, -0.8284,  ..., -1.7870, -1.7870, -1.7696],\n",
      "          [-0.7413, -0.7238, -0.6541,  ..., -1.7696, -1.7696, -1.7522],\n",
      "          ...,\n",
      "          [-0.5670,  0.3393,  0.8797,  ..., -1.6999, -1.6824, -1.6999],\n",
      "          [-0.4275,  0.3916,  0.8971,  ..., -1.6824, -1.6999, -1.6476],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          [-0.4275,  0.4788,  0.9319,  ..., -1.7347, -1.7522, -1.7173]]]])\n",
      "captions: tensor([[   0,    3,  371,  224,   39,  160,  325,  364,  161,   47,  327,  786,\n",
      "           18,    1],\n",
      "        [   0,    3,   91,  224,  111,    3,  112,  360,    3,  925,   13, 2076,\n",
      "           18,    1],\n",
      "        [   0,  253,    6,  239, 1548,  509,    3, 1815,   77,  121,   13,    3,\n",
      "          137,    1],\n",
      "        [   0,   47,  327,  786,  332,   21,    3,  985,  569,  407,   24, 1165,\n",
      "           18,    1],\n",
      "        [   0,    3,  115,    6,    3,  371,  324,   54,  364,  161,   72,  409,\n",
      "           18,    1],\n",
      "        [   0,    3,  169, 3381,   86,  360,    3, 3381, 1251,  537,   21, 3864,\n",
      "           18,    1],\n",
      "        [   0,    3,  286,  294,  460,   32,  665,  161, 1612,    3, 1492,  353,\n",
      "           18,    1],\n",
      "        [   0,  250,   52,    2,   86, 1372,   86,    6,    2,   77,  682, 4542,\n",
      "           18,    1],\n",
      "        [   0,    3,  578,  270,  160,  899,   39,    3,   40,   79,   32, 5262,\n",
      "           18,    1],\n",
      "        [   0,    3,  647,  294,   13,    3,  272, 2711,   73, 2458,   13, 1198,\n",
      "           18,    1]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Randomly sample a caption length, and sample indices with that length.\n",
    "indices = data_loader.dataset.get_train_indices()\n",
    "print('sampled indices:', indices)\n",
    "\n",
    "# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "images, captions = next(iter(data_loader))\n",
    "    \n",
    "print('images.shape:', images.shape)\n",
    "print('captions.shape:', captions.shape)\n",
    "\n",
    "print('images:', images)\n",
    "print('captions:', captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeSJV15cCnkd"
   },
   "source": [
    "## 3: Building the CNN Encoder\n",
    "* The CNN Encoder is defined in ``models.py``\n",
    "* We import `EncoderCNN` and `DecoderRNN` from ``model.py``. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9ETz2a4Cnkp"
   },
   "source": [
    "The encoder uses the pre-trained ResNet-50 architecture (with the final fully-connected layer removed) to extract features from a batch of pre-processed images.  The output is then flattened to a vector, before being passed through a `Linear` layer to transform the feature vector to have the same size as the word embedding.\n",
    "\n",
    "![Encoder](https://github.com/varangrai/Udacity-CVND-Projects/blob/master/Image%20Captioning/images/encoder.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEAmHvkbCnke"
   },
   "outputs": [],
   "source": [
    "from model import EncoderCNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GgzDjc5KCnkm"
   },
   "source": [
    "Run the code cell below to instantiate the CNN encoder in `encoder`.  \n",
    "\n",
    "The pre-processed images from the batch in **Step 2** of this notebook are then passed through the encoder, and the output is stored in `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJ6xcoHKCnkn",
    "outputId": "6dc99f38-721b-4b76-bcd7-bd1934ac4945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(features): <class 'torch.Tensor'>\n",
      "features.shape: torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "embed_size = 256      #dimensionality of the image embedding.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "encoder.to(device)\n",
    "images = images.to(device)\n",
    "features = encoder(images)\n",
    "\n",
    "print('type(features):', type(features))\n",
    "print('features.shape:', features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wht4J6xfCnkq"
   },
   "source": [
    "## 4: RNN Decoder\n",
    "\n",
    "The decoder accepts an arbitrary batch (of embedded image features and pre-processed captions [where all captions have the same length]) as input.  \n",
    "\n",
    "* The decoder returns `outputs` which has shape `[batch_size, captions.shape[1], vocab_size]` and `outputs[i,j,k]` contains the model's predicted score, indicating how likely the `j`-th token in the `i`-th caption in the batch is the `k`-th token in the vocabulary.\n",
    "\n",
    "\n",
    "![Decoder](https://github.com/varangrai/Udacity-CVND-Projects/blob/master/Image%20Captioning/images/encoder-decoder.png?raw=true)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhInoz8-Tyxo"
   },
   "outputs": [],
   "source": [
    "from model import DecoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DzQe4FSRCnkq",
    "outputId": "06a934a0-7e08-484d-c881-30894a124695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(outputs): <class 'torch.Tensor'>\n",
      "expected output: [ 10 , 14 , 9955 ]\n",
      "outputs.shape: torch.Size([10, 14, 9955])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "decoder.to(device)\n",
    "captions = captions.to(device)\n",
    "outputs = decoder(features, captions)\n",
    "\n",
    "print('type(outputs):', type(outputs))\n",
    "print('expected output: [', batch_size, ',', captions.shape[1], ',', vocab_size, ']')\n",
    "print('outputs.shape:', outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary:\n",
    "\n",
    "- Pre-trained resnet50,is used as an Encoder provding an an embedding of size 256\n",
    "- An LSTM is used as a decoder, with embed size of 256 and hidden size of 512, following the [paper](https://arxiv.org/pdf/1411.4555.pdf).\n",
    "- Vocabulary threshold used is 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "1_Preliminaries.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
