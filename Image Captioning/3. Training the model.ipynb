{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "2_Training.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYqyQ187VBXn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Notebook 3: Training the CNN-RNN model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj-upYVEVBXr",
        "colab_type": "text"
      },
      "source": [
        "<a id='step1'></a>\n",
        "## 1: Training Preparation\n",
        "\n",
        "* ``params``: It is a Python list containing the learnable parameters of the model. \n",
        "- Only the weights in the embedding layer of the encoder and the weights of the decoder were trained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHuWO1gVVBXt",
        "colab_type": "code",
        "colab": {},
        "outputId": "abc7bd74-97ae-410c-ff12-2c038a10a4f0"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import sys\n",
        "sys.path.append('/opt/cocoapi/PythonAPI')\n",
        "from pycocotools.coco import COCO\n",
        "from data_loader import get_loader\n",
        "from model import EncoderCNN, DecoderRNN\n",
        "import math\n",
        "\n",
        "batch_size = 128           \n",
        "vocab_threshold = 4        # minimum word count threshold\n",
        "vocab_from_file = True    \n",
        "embed_size = 256           # dimensionality of image and word embeddings\n",
        "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
        "num_epochs = 3             # number of training epochs\n",
        "save_every = 1             # determines frequency of saving model weights\n",
        "print_every = 100          # determines window for printing average loss\n",
        "\n",
        "transform_train = transforms.Compose([ \n",
        "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
        "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
        "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
        "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "# Build data loader.\n",
        "data_loader = get_loader(transform=transform_train,\n",
        "                         mode='train',\n",
        "                         batch_size=batch_size,\n",
        "                         vocab_threshold=vocab_threshold,\n",
        "                         vocab_from_file=vocab_from_file)\n",
        "\n",
        "vocab_size = len(data_loader.dataset.vocab)\n",
        "\n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
        "optimizer = torch.optim.Adam(params, lr=0.001) \n",
        "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.30s)\n",
            "creating index...\n",
            "index created!\n",
            "[0/414113] Tokenizing captions...\n",
            "[100000/414113] Tokenizing captions...\n",
            "[200000/414113] Tokenizing captions...\n",
            "[300000/414113] Tokenizing captions...\n",
            "[400000/414113] Tokenizing captions...\n",
            "loading annotations into memory...\n",
            "Done (t=0.81s)\n",
            "creating index...\n",
            "index created!\n",
            "Obtaining caption lengths...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████| 414113/414113 [00:48<00:00, 8512.66it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHJs3sJbVBX4",
        "colab_type": "text"
      },
      "source": [
        "## 2: Training the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KovPR4enVBX5",
        "colab_type": "code",
        "colab": {},
        "outputId": "1f233011-efa3-405f-9ce6-8de876e5802f"
      },
      "source": [
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    for i_step in range(1, total_step+1):\n",
        "        \n",
        "        # Randomly sample a caption length, and sample indices with that length.\n",
        "        indices = data_loader.dataset.get_train_indices()\n",
        "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "        data_loader.batch_sampler.sampler = new_sampler\n",
        "        \n",
        "        # Obtain the batch.\n",
        "        images, captions = next(iter(data_loader))\n",
        "\n",
        "        # Move batch of images and captions to GPU if CUDA is available.\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "        \n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "            \n",
        "        # Get training statistics.\n",
        "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
        "        \n",
        "        print('\\r' + stats, end=\"\")\n",
        "        \n",
        "        if i_step % print_every == 0:\n",
        "            print('\\r' + stats)\n",
        "            \n",
        "    # Save the weights.\n",
        "    if epoch % save_every == 0:\n",
        "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
        "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Step [100/3236], Loss: 3.7985, Perplexity: 44.6321\n",
            "Epoch [1/3], Step [200/3236], Loss: 3.5927, Perplexity: 36.33101\n",
            "Epoch [1/3], Step [300/3236], Loss: 3.1750, Perplexity: 23.9261\n",
            "Epoch [1/3], Step [400/3236], Loss: 3.2486, Perplexity: 25.7532\n",
            "Epoch [1/3], Step [500/3236], Loss: 3.3328, Perplexity: 28.01535\n",
            "Epoch [1/3], Step [600/3236], Loss: 2.8008, Perplexity: 16.4580\n",
            "Epoch [1/3], Step [700/3236], Loss: 3.0129, Perplexity: 20.3461\n",
            "Epoch [1/3], Step [800/3236], Loss: 2.7014, Perplexity: 14.9004\n",
            "Epoch [1/3], Step [900/3236], Loss: 2.8401, Perplexity: 17.1173\n",
            "Epoch [1/3], Step [1000/3236], Loss: 2.4991, Perplexity: 12.1720\n",
            "Epoch [1/3], Step [1100/3236], Loss: 2.8928, Perplexity: 18.0430\n",
            "Epoch [1/3], Step [1200/3236], Loss: 2.3459, Perplexity: 10.4429\n",
            "Epoch [1/3], Step [1300/3236], Loss: 2.4772, Perplexity: 11.9076\n",
            "Epoch [1/3], Step [1400/3236], Loss: 2.4438, Perplexity: 11.5164\n",
            "Epoch [1/3], Step [1500/3236], Loss: 2.3213, Perplexity: 10.1889\n",
            "Epoch [1/3], Step [1600/3236], Loss: 2.4267, Perplexity: 11.3213\n",
            "Epoch [1/3], Step [1700/3236], Loss: 2.4094, Perplexity: 11.1271\n",
            "Epoch [1/3], Step [1800/3236], Loss: 2.5120, Perplexity: 12.3290\n",
            "Epoch [1/3], Step [1900/3236], Loss: 2.8461, Perplexity: 17.2213\n",
            "Epoch [1/3], Step [2000/3236], Loss: 2.2990, Perplexity: 9.96463\n",
            "Epoch [1/3], Step [2100/3236], Loss: 2.1831, Perplexity: 8.87365\n",
            "Epoch [1/3], Step [2200/3236], Loss: 2.2525, Perplexity: 9.51184\n",
            "Epoch [1/3], Step [2300/3236], Loss: 2.4056, Perplexity: 11.0856\n",
            "Epoch [1/3], Step [2400/3236], Loss: 2.6142, Perplexity: 13.6561\n",
            "Epoch [1/3], Step [2500/3236], Loss: 2.5264, Perplexity: 12.5085\n",
            "Epoch [1/3], Step [2600/3236], Loss: 2.3673, Perplexity: 10.6688\n",
            "Epoch [1/3], Step [2700/3236], Loss: 2.1211, Perplexity: 8.34035\n",
            "Epoch [1/3], Step [2800/3236], Loss: 2.3033, Perplexity: 10.0068\n",
            "Epoch [1/3], Step [2900/3236], Loss: 2.1785, Perplexity: 8.83298\n",
            "Epoch [1/3], Step [3000/3236], Loss: 2.1958, Perplexity: 8.98729\n",
            "Epoch [1/3], Step [3100/3236], Loss: 2.1569, Perplexity: 8.64401\n",
            "Epoch [1/3], Step [3200/3236], Loss: 2.0749, Perplexity: 7.96401\n",
            "Epoch [2/3], Step [100/3236], Loss: 2.2450, Perplexity: 9.440689\n",
            "Epoch [2/3], Step [200/3236], Loss: 2.0759, Perplexity: 7.97145\n",
            "Epoch [2/3], Step [300/3236], Loss: 2.1578, Perplexity: 8.65230\n",
            "Epoch [2/3], Step [400/3236], Loss: 2.1288, Perplexity: 8.40441\n",
            "Epoch [2/3], Step [500/3236], Loss: 2.0936, Perplexity: 8.11384\n",
            "Epoch [2/3], Step [600/3236], Loss: 2.2003, Perplexity: 9.02797\n",
            "Epoch [2/3], Step [700/3236], Loss: 2.1690, Perplexity: 8.74916\n",
            "Epoch [2/3], Step [800/3236], Loss: 2.1566, Perplexity: 8.64141\n",
            "Epoch [2/3], Step [900/3236], Loss: 2.1636, Perplexity: 8.70285\n",
            "Epoch [2/3], Step [1000/3236], Loss: 2.0381, Perplexity: 7.6759\n",
            "Epoch [2/3], Step [1100/3236], Loss: 2.4803, Perplexity: 11.9450\n",
            "Epoch [2/3], Step [1200/3236], Loss: 2.3653, Perplexity: 10.6469\n",
            "Epoch [2/3], Step [1300/3236], Loss: 2.1355, Perplexity: 8.46154\n",
            "Epoch [2/3], Step [1400/3236], Loss: 2.4711, Perplexity: 11.8350\n",
            "Epoch [2/3], Step [1500/3236], Loss: 2.0747, Perplexity: 7.96241\n",
            "Epoch [2/3], Step [1600/3236], Loss: 2.1747, Perplexity: 8.79956\n",
            "Epoch [2/3], Step [1700/3236], Loss: 2.1338, Perplexity: 8.44708\n",
            "Epoch [2/3], Step [1800/3236], Loss: 2.1001, Perplexity: 8.16687\n",
            "Epoch [2/3], Step [1900/3236], Loss: 2.1294, Perplexity: 8.41013\n",
            "Epoch [2/3], Step [2000/3236], Loss: 1.9830, Perplexity: 7.26447\n",
            "Epoch [2/3], Step [2100/3236], Loss: 2.1101, Perplexity: 8.24897\n",
            "Epoch [2/3], Step [2200/3236], Loss: 2.0987, Perplexity: 8.15562\n",
            "Epoch [2/3], Step [2300/3236], Loss: 1.9685, Perplexity: 7.15963\n",
            "Epoch [2/3], Step [2400/3236], Loss: 2.0465, Perplexity: 7.74104\n",
            "Epoch [2/3], Step [2500/3236], Loss: 2.0809, Perplexity: 8.01159\n",
            "Epoch [2/3], Step [2600/3236], Loss: 2.0287, Perplexity: 7.60420\n",
            "Epoch [2/3], Step [2700/3236], Loss: 2.0800, Perplexity: 8.00425\n",
            "Epoch [2/3], Step [2800/3236], Loss: 2.2557, Perplexity: 9.54199\n",
            "Epoch [2/3], Step [2900/3236], Loss: 2.3992, Perplexity: 11.0146\n",
            "Epoch [2/3], Step [3000/3236], Loss: 2.0865, Perplexity: 8.05706\n",
            "Epoch [2/3], Step [3100/3236], Loss: 1.9245, Perplexity: 6.85140\n",
            "Epoch [2/3], Step [3200/3236], Loss: 2.0666, Perplexity: 7.89819\n",
            "Epoch [3/3], Step [100/3236], Loss: 2.0136, Perplexity: 7.489916\n",
            "Epoch [3/3], Step [200/3236], Loss: 1.8568, Perplexity: 6.40294\n",
            "Epoch [3/3], Step [300/3236], Loss: 2.0356, Perplexity: 7.65695\n",
            "Epoch [3/3], Step [400/3236], Loss: 1.8873, Perplexity: 6.60134\n",
            "Epoch [3/3], Step [500/3236], Loss: 2.0006, Perplexity: 7.39343\n",
            "Epoch [3/3], Step [600/3236], Loss: 1.9708, Perplexity: 7.17654\n",
            "Epoch [3/3], Step [700/3236], Loss: 2.0232, Perplexity: 7.56281\n",
            "Epoch [3/3], Step [800/3236], Loss: 2.1288, Perplexity: 8.40442\n",
            "Epoch [3/3], Step [900/3236], Loss: 1.9585, Perplexity: 7.08850\n",
            "Epoch [3/3], Step [1000/3236], Loss: 1.9820, Perplexity: 7.2574\n",
            "Epoch [3/3], Step [1100/3236], Loss: 1.9478, Perplexity: 7.01323\n",
            "Epoch [3/3], Step [1200/3236], Loss: 1.9303, Perplexity: 6.89157\n",
            "Epoch [3/3], Step [1300/3236], Loss: 1.9376, Perplexity: 6.94205\n",
            "Epoch [3/3], Step [1400/3236], Loss: 2.5084, Perplexity: 12.2859\n",
            "Epoch [3/3], Step [1500/3236], Loss: 1.8645, Perplexity: 6.45247\n",
            "Epoch [3/3], Step [1600/3236], Loss: 1.9330, Perplexity: 6.91003\n",
            "Epoch [3/3], Step [1700/3236], Loss: 2.0429, Perplexity: 7.71302\n",
            "Epoch [3/3], Step [1800/3236], Loss: 1.8365, Perplexity: 6.27481\n",
            "Epoch [3/3], Step [1900/3236], Loss: 1.8616, Perplexity: 6.43412\n",
            "Epoch [3/3], Step [2000/3236], Loss: 1.8994, Perplexity: 6.68219\n",
            "Epoch [3/3], Step [2100/3236], Loss: 2.6703, Perplexity: 14.4444\n",
            "Epoch [3/3], Step [2200/3236], Loss: 2.1569, Perplexity: 8.64459\n",
            "Epoch [3/3], Step [2300/3236], Loss: 1.9281, Perplexity: 6.87610\n",
            "Epoch [3/3], Step [2400/3236], Loss: 2.0635, Perplexity: 7.87344\n",
            "Epoch [3/3], Step [2500/3236], Loss: 1.9365, Perplexity: 6.93425\n",
            "Epoch [3/3], Step [2600/3236], Loss: 1.9298, Perplexity: 6.88832\n",
            "Epoch [3/3], Step [2700/3236], Loss: 2.5855, Perplexity: 13.2704\n",
            "Epoch [3/3], Step [2800/3236], Loss: 2.5699, Perplexity: 13.0641\n",
            "Epoch [3/3], Step [2900/3236], Loss: 1.9656, Perplexity: 7.13922\n",
            "Epoch [3/3], Step [3000/3236], Loss: 3.0035, Perplexity: 20.1559\n",
            "Epoch [3/3], Step [3100/3236], Loss: 1.8254, Perplexity: 6.20557\n",
            "Epoch [3/3], Step [3200/3236], Loss: 1.9332, Perplexity: 6.91182\n",
            "Epoch [3/3], Step [3236/3236], Loss: 1.8850, Perplexity: 6.58647"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}